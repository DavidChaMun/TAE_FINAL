{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "67rwmE6j_bzx"
   },
   "source": [
    "## Red neuronal utilizando Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_PFlBms_bz0"
   },
   "source": [
    "Ahora construiremos una red neuronal utilizando la libreria de Keras que imprementa tensorflow.\n",
    "Utilizaremos la base de datos sin valores faltantes y con las modificaciones mencionadas previamente para entrenar el modelo. El modelo se elegira mediante validación cruzada, para probar su efectividad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDgVN0vl_bz1"
   },
   "source": [
    "### Cargando librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0HOEBZP_bz2"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import os\n",
    "import functools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_s3hfTTHqvU"
   },
   "source": [
    "Cargamos la base de datos desde Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0Vho-SP_bz5"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('https://github.com/DavidChaMun/TAE_FINAL/raw/master/data/dataset_tae_final_no_na_mod.csv')\n",
    "test = pd.read_csv('https://github.com/DavidChaMun/TAE_FINAL/raw/master/data/test_tae_no_na_mod.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0k8H59A_b0D"
   },
   "source": [
    "Observemos la estructura de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "gLCnuCIZ_b0E",
    "outputId": "74e13942-49cf-4122-a7d8-c1b249f27c87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  int64\n",
       "workclass         category\n",
       "fnlwgt               int64\n",
       "education         category\n",
       "marital_status    category\n",
       "ocupation         category\n",
       "ethnicity         category\n",
       "gender            category\n",
       "capital_gain         int64\n",
       "capital_loss         int64\n",
       "hours_per_week       int64\n",
       "native_country    category\n",
       "income                int8\n",
       "dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chequeo de tipos de las variables\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_6aai8RV9SD"
   },
   "source": [
    "## Transformación de la base de datos a un formato conveniente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0tGLQMQ_b0H"
   },
   "source": [
    "Vemos que tenemos variables categóricas, debemos transformarlas a variables dummies para poder crear la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygOZN5ay_b0I"
   },
   "outputs": [],
   "source": [
    "catego_columns = ['education', 'workclass', 'marital_status', \n",
    "             'ethnicity', 'income','gender',\n",
    "                  'native_country', 'ocupation']\n",
    "#Transformamos variables object a categoricas\n",
    "for col in catego_columns:\n",
    "    train[col] = pd.Categorical(train[col])\n",
    "    test[col] = pd.Categorical(test[col])\n",
    "\n",
    "#Transformamos nuestra variable de objetivo para la clasificacion\n",
    "train[label] = train[label].cat.codes\n",
    "test[label] = test[label].cat.codes\n",
    "\n",
    "#Creamos variables dummies con las categoricas\n",
    "train_dataset=pd.get_dummies(train)\n",
    "test_dataset=pd.get_dummies(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZjJpPyNS_b0K"
   },
   "source": [
    "Chequeamos que las variables de nuestro conjunto de entrenamiento sean las mismas que las de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_DvL0wTc_b0L"
   },
   "outputs": [],
   "source": [
    "for elem in list(train_dataset.columns):\n",
    "    if not((elem in list(test_dataset.columns))):\n",
    "        print(elem)\n",
    "#Dado que no imprime nada, las dos bases tienen las mismas variables dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "s3kxDMNM_b0O",
    "outputId": "3a1d1989-f003-4970-9104-6db9036fe23f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'fnlwgt', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
       "       'income', 'workclass_ Federal-gov', 'workclass_ Local-gov',\n",
       "       'workclass_ Private', 'workclass_ Self-emp-inc',\n",
       "       'workclass_ Self-emp-not-inc', 'workclass_ State-gov',\n",
       "       'workclass_ Without-pay', 'education_ 10th', 'education_ 11th',\n",
       "       'education_ 12th', 'education_ 1st-4th', 'education_ 5th-6th',\n",
       "       'education_ 7th-8th', 'education_ 9th', 'education_ Assoc-acdm',\n",
       "       'education_ Assoc-voc', 'education_ Bachelors', 'education_ Doctorate',\n",
       "       'education_ HS-grad', 'education_ Masters', 'education_ Preschool',\n",
       "       'education_ Prof-school', 'education_ Some-college',\n",
       "       'marital_status_ Divorced', 'marital_status_ Never-married',\n",
       "       'marital_status_ Separated', 'marital_status_ Widowed',\n",
       "       'marital_status_Married', 'ocupation_ Adm-clerical',\n",
       "       'ocupation_ Armed-Forces', 'ocupation_ Craft-repair',\n",
       "       'ocupation_ Exec-managerial', 'ocupation_ Farming-fishing',\n",
       "       'ocupation_ Handlers-cleaners', 'ocupation_ Machine-op-inspct',\n",
       "       'ocupation_ Other-service', 'ocupation_ Priv-house-serv',\n",
       "       'ocupation_ Prof-specialty', 'ocupation_ Protective-serv',\n",
       "       'ocupation_ Sales', 'ocupation_ Tech-support',\n",
       "       'ocupation_ Transport-moving', 'ethnicity_ Amer-Indian-Eskimo',\n",
       "       'ethnicity_ Asian-Pac-Islander', 'ethnicity_ Black', 'ethnicity_ Other',\n",
       "       'ethnicity_ White', 'gender_ Female', 'gender_ Male',\n",
       "       'native_country_ Mexico', 'native_country_ United-States',\n",
       "       'native_country_other'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = train_dataset.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUFbvvMo_b0Y"
   },
   "outputs": [],
   "source": [
    "#train_dataset.to_csv('train_dummies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnSvTSC0_b0a"
   },
   "source": [
    "# Creando la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9LM5kz7PXPD"
   },
   "source": [
    "Para la construcción de la red neuronal se utilizo el modelo *sequential* de Keras, la cuál ajusta una red neuronal (utilizando ciertos métodos que describiremos más adelante), dicha función se entrena utilizando *.fit()*, este método recibe un conjunto de entrenamiento y permite introducir un conjunto de validación, por lo que en cada iteración de entrenamiento se puede ver la precisión de la predicción para ambos conjuntos de datos. \n",
    "\n",
    "Se eligió como criterio de elección del modelo aquel que maximizara la precisión en las predicciones de la variables *income*. Para la elección del modelo se variaron los siguientes atributos:\n",
    "\n",
    "- Número de capas y neuronas dentro de *sequential*: aumentar el numero de neuronas a más de dos no mejoró los resultados, el número de neuronas mejoraba los resultados al aumentarlas por encimas de 10, al final se eligieron 2 capas internas con 30 y 20 neuronas respectivamentes.\n",
    "\n",
    "- Método de activación de la neurona de salida: se probo \"softmax\" que según la documentación de internet es eficiente en clasificación de 2 o multiples variables categóricas. Sin embargo, este mostro precisiones desastrozas por debajo de 0.30 en general. Por lo que se utilizo \"sigmoid\".\n",
    "\n",
    "- Optimizador: se probaron *SGD* (precisiones de hasta 0.83), *Adam* y *RMSprop*, variando el __learning rate (LR)__ en todos ellos, se encontro que *Adam* y *RMSprop* llegaban a resultados equivalente de hasta 0.85 de precisión, los LR entre 0.001 y 0.1 convergian siempre a una precisión de 0.85, por lo que se eligió 0.1 al entrenar la red más rapidamente.\n",
    "\n",
    "- Número de epochs: estos son definimos al usar *model.fit()*. Aumentarlos o disminuirlos no impacto mucho el modelo final, la neurona convergia para el 10avo epoch, se eligio 20 para visualizar un comportamiento más prolongado.\n",
    "\n",
    "### A continuación se muestra la red neuronal final construida:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jk3ul1zaWLGn"
   },
   "source": [
    "#### Definición de la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33wE3WM4_b0b"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import adam\n",
    "\n",
    "#Función que crea la red\n",
    "def get_compiled_model():\n",
    "  #Modelo sequencial, posee las neuronas\n",
    "  model = Sequential([\n",
    "    Dense(40, input_shape=(x_train.shape[1:])),\n",
    "    Activation('relu'),\n",
    "    Dense(20),\n",
    "    Activation('relu'),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid'), #última neuronal\n",
    "  ])\n",
    "  \n",
    "  #optimizador\n",
    "  opt= adam(lr=0.01,beta_1=0.9, beta_2=0.9)\n",
    "  \n",
    "  #definición del copiador\n",
    "  model.compile(optimizer=opt,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksAk35dEWRTJ"
   },
   "source": [
    "#### Escalado y definición de variables de apoyo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvK6XI3k6UhS"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Definimos las variables de entrenamiento y las objetivo para entrenar el modelo\n",
    "x_train = train_dataset.copy(deep=True)\n",
    "x_val = test_dataset.copy(deep=True)\n",
    "y_train = x_train.pop('income').values\n",
    "y_val = x_val.pop('income').values\n",
    "\n",
    "#Escalamos los datos\n",
    "scaler = MinMaxScaler() #definimos nuestro escalador, nos será útil a la hora de convertir los datos para hacer las predicciones.\n",
    "scaler.fit(x_train)    \n",
    "x_train =  scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fp6mfoNcWXaG"
   },
   "source": [
    "### Entrenamiento y validación de la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "colab_type": "code",
    "id": "lX1Q9Ye4_b0e",
    "outputId": "392c809b-f973-436e-ff4a-28bcbefd4ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30160 samples, validate on 15059 samples\n",
      "Epoch 1/20\n",
      "30160/30160 [==============================] - 4s 121us/step - loss: 0.3632 - acc: 0.8313 - val_loss: 0.3498 - val_acc: 0.8375\n",
      "Epoch 2/20\n",
      "30160/30160 [==============================] - 3s 83us/step - loss: 0.3440 - acc: 0.8404 - val_loss: 0.3464 - val_acc: 0.8335\n",
      "Epoch 3/20\n",
      "30160/30160 [==============================] - 3s 84us/step - loss: 0.3412 - acc: 0.8440 - val_loss: 0.3353 - val_acc: 0.8447\n",
      "Epoch 4/20\n",
      "30160/30160 [==============================] - 3s 84us/step - loss: 0.3386 - acc: 0.8457 - val_loss: 0.3348 - val_acc: 0.8465\n",
      "Epoch 5/20\n",
      "30160/30160 [==============================] - 2s 82us/step - loss: 0.3350 - acc: 0.8473 - val_loss: 0.3400 - val_acc: 0.8443\n",
      "Epoch 6/20\n",
      "30160/30160 [==============================] - 3s 84us/step - loss: 0.3353 - acc: 0.8488 - val_loss: 0.3373 - val_acc: 0.8475\n",
      "Epoch 7/20\n",
      "30160/30160 [==============================] - 3s 84us/step - loss: 0.3346 - acc: 0.8487 - val_loss: 0.3323 - val_acc: 0.8483\n",
      "Epoch 8/20\n",
      "30160/30160 [==============================] - 3s 84us/step - loss: 0.3339 - acc: 0.8507 - val_loss: 0.3367 - val_acc: 0.8477\n",
      "Epoch 9/20\n",
      "30160/30160 [==============================] - 3s 85us/step - loss: 0.3330 - acc: 0.8508 - val_loss: 0.3400 - val_acc: 0.8489\n",
      "Epoch 10/20\n",
      "30160/30160 [==============================] - 3s 85us/step - loss: 0.3330 - acc: 0.8507 - val_loss: 0.3350 - val_acc: 0.8491\n",
      "Epoch 11/20\n",
      "30160/30160 [==============================] - 3s 84us/step - loss: 0.3329 - acc: 0.8513 - val_loss: 0.3467 - val_acc: 0.8446\n",
      "Epoch 12/20\n",
      "30160/30160 [==============================] - 3s 85us/step - loss: 0.3329 - acc: 0.8518 - val_loss: 0.3405 - val_acc: 0.8491\n",
      "Epoch 13/20\n",
      "30160/30160 [==============================] - 3s 84us/step - loss: 0.3350 - acc: 0.8521 - val_loss: 0.3464 - val_acc: 0.8479\n",
      "Epoch 14/20\n",
      "30160/30160 [==============================] - 3s 85us/step - loss: 0.3329 - acc: 0.8523 - val_loss: 0.3494 - val_acc: 0.8384\n",
      "Epoch 15/20\n",
      "30160/30160 [==============================] - 3s 85us/step - loss: 0.3342 - acc: 0.8528 - val_loss: 0.3606 - val_acc: 0.8477\n",
      "Epoch 16/20\n",
      "30160/30160 [==============================] - 3s 85us/step - loss: 0.3351 - acc: 0.8524 - val_loss: 0.3440 - val_acc: 0.8437\n",
      "Epoch 17/20\n",
      "30160/30160 [==============================] - 3s 84us/step - loss: 0.3347 - acc: 0.8528 - val_loss: 0.3480 - val_acc: 0.8485\n",
      "Epoch 18/20\n",
      "30160/30160 [==============================] - 3s 84us/step - loss: 0.3334 - acc: 0.8540 - val_loss: 0.3451 - val_acc: 0.8474\n",
      "Epoch 19/20\n",
      "30160/30160 [==============================] - 3s 83us/step - loss: 0.3350 - acc: 0.8542 - val_loss: 0.3411 - val_acc: 0.8481\n",
      "Epoch 20/20\n",
      "30160/30160 [==============================] - 2s 83us/step - loss: 0.3357 - acc: 0.8538 - val_loss: 0.3407 - val_acc: 0.8502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff5d11160f0>"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Obtenemos nuestro modelo\n",
    "model = get_compiled_model()\n",
    "\n",
    "#Entrenamos el modelo.\n",
    "model.fit(x=x_train, y=y_train,validation_data=(x_val,y_val),epochs = 20,batch_size=20,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyfzJTfrV7O9"
   },
   "source": [
    "Nuestro modelo final tiene una precisión del **~85%** en los datos de validación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUTi7pwJW2wz"
   },
   "source": [
    "### Demo de nuevas predicciones\n",
    "\n",
    "Ahora construiremos una función que reciba una base numpy con los datos a predecir, construya las variables dummies, las escale y sea predicho por el modelo final.\n",
    "\n",
    "Para conseguir la misma estructura de la base de datos con la que se entreno el modelo, basta con crear un pd.data_frame con valores de un _input valido_, y añadir las columnas de la df con dummies que falten con valores con cero. Luego se escala con el objeto _scaler_ con el que se escaló la base original. \n",
    "\n",
    "Una clase que realiza este proceso cargando la red neuronal y el scaler se presenta a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_DG-ceXGVOY"
   },
   "outputs": [],
   "source": [
    "#Librerias necesarias:\n",
    "from sklearn.externals import joblib\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "class neural_network_mod():\n",
    "\n",
    "    scaler =None\n",
    "    nn_model = None\n",
    "    train_cols = ['age', 'fnlwgt', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "       'workclass_ Federal-gov', 'workclass_ Local-gov', 'workclass_ Private',\n",
    "       'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc',\n",
    "       'workclass_ State-gov', 'workclass_ Without-pay', 'education_ 10th',\n",
    "       'education_ 11th', 'education_ 12th', 'education_ 1st-4th',\n",
    "       'education_ 5th-6th', 'education_ 7th-8th', 'education_ 9th',\n",
    "       'education_ Assoc-acdm', 'education_ Assoc-voc', 'education_ Bachelors',\n",
    "       'education_ Doctorate', 'education_ HS-grad', 'education_ Masters',\n",
    "       'education_ Preschool', 'education_ Prof-school',\n",
    "       'education_ Some-college', 'marital_status_ Divorced',\n",
    "       'marital_status_ Never-married', 'marital_status_ Separated',\n",
    "       'marital_status_ Widowed', 'marital_status_Married',\n",
    "       'ocupation_ Adm-clerical', 'ocupation_ Armed-Forces',\n",
    "       'ocupation_ Craft-repair', 'ocupation_ Exec-managerial',\n",
    "       'ocupation_ Farming-fishing', 'ocupation_ Handlers-cleaners',\n",
    "       'ocupation_ Machine-op-inspct', 'ocupation_ Other-service',\n",
    "       'ocupation_ Priv-house-serv', 'ocupation_ Prof-specialty',\n",
    "       'ocupation_ Protective-serv', 'ocupation_ Sales',\n",
    "       'ocupation_ Tech-support', 'ocupation_ Transport-moving',\n",
    "       'ethnicity_ Amer-Indian-Eskimo', 'ethnicity_ Asian-Pac-Islander',\n",
    "       'ethnicity_ Black', 'ethnicity_ Other', 'ethnicity_ White',\n",
    "       'gender_ Female', 'gender_ Male', 'native_country_ Mexico',\n",
    "       'native_country_ United-States', 'native_country_other']\n",
    "    catego_columns = ['education', 'workclass', 'marital_status', \n",
    "             'ethnicity','gender',\n",
    "                  'native_country', 'ocupation']\n",
    "    \n",
    "    def __init__(self):\n",
    "        #Carga el scaler y la red neuronal:\n",
    "        self.scaler = joblib.load('scaler.save')\n",
    "        self.nn_model = load_model('my_model.h5')\n",
    "        \n",
    "    def predict(self, data):\n",
    "      #Esta función s eutiliza para predecir, recibe data un pd_dataframe\n",
    "      \n",
    "      #Transformamos la base de datos\n",
    "      data = self.tidy_data(data)\n",
    "      \n",
    "      predictions = self.nn_model.predict(data)>0.5\n",
    "      predictions=np.where(predictions==0, \"<=50k\", predictions)\n",
    "      predictions=np.where(predictions=='True', \">50k\", predictions)\n",
    "      return(predictions)\n",
    "      \n",
    "    def tidy_data(self,data):\n",
    "      \n",
    "      #Transformamos variables object a categoricas\n",
    "      for col in self.catego_columns:        \n",
    "        data[col] = pd.Categorical(data[col])\n",
    "      \n",
    "      #Creamos variables dummies con las categoricas\n",
    "      data_d=pd.get_dummies(data)\n",
    "      \n",
    "      #Rellenamos con las variables dummies\n",
    "      missing_cols = set(self.train_cols) - set(data_d.columns)\n",
    "      \n",
    "      for c in missing_cols:\n",
    "        data_d[c] = 0\n",
    "        \n",
    "      # Ensure the order of column in the test set is in the same order than in train set\n",
    "      data_d = data_d[self.train_cols]\n",
    "      \n",
    "      #Escalamos los datos\n",
    "      data_d = self.scaler.transform(data_d)    \n",
    "      return(data_d)\n",
    "      \n",
    " \n",
    "\n",
    "\n",
    "ob = neural_network_mod()\n",
    "ob.predict(new_pred)\n",
    "#model.predict(x_train[[1,2],:])>0.5\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JV94lsw2ZmJF"
   },
   "source": [
    "### Guardando el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qi10Q0YbZleg"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "# Creates a HDF5 file 'my_model.h5'\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# Deletes the existing model\n",
    "#del model  \n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "model2 = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEI7DUqAmqsN"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "ojrGTnUwmxsi",
    "outputId": "50302124-c624-4b80-aff9-1214b2a02547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'fnlwgt', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
       "       'workclass_ Federal-gov', 'workclass_ Local-gov', 'workclass_ Private',\n",
       "       'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc',\n",
       "       'workclass_ State-gov', 'workclass_ Without-pay', 'education_ 10th',\n",
       "       'education_ 11th', 'education_ 12th', 'education_ 1st-4th',\n",
       "       'education_ 5th-6th', 'education_ 7th-8th', 'education_ 9th',\n",
       "       'education_ Assoc-acdm', 'education_ Assoc-voc', 'education_ Bachelors',\n",
       "       'education_ Doctorate', 'education_ HS-grad', 'education_ Masters',\n",
       "       'education_ Preschool', 'education_ Prof-school',\n",
       "       'education_ Some-college', 'marital_status_ Divorced',\n",
       "       'marital_status_ Never-married', 'marital_status_ Separated',\n",
       "       'marital_status_ Widowed', 'marital_status_Married',\n",
       "       'ocupation_ Adm-clerical', 'ocupation_ Armed-Forces',\n",
       "       'ocupation_ Craft-repair', 'ocupation_ Exec-managerial',\n",
       "       'ocupation_ Farming-fishing', 'ocupation_ Handlers-cleaners',\n",
       "       'ocupation_ Machine-op-inspct', 'ocupation_ Other-service',\n",
       "       'ocupation_ Priv-house-serv', 'ocupation_ Prof-specialty',\n",
       "       'ocupation_ Protective-serv', 'ocupation_ Sales',\n",
       "       'ocupation_ Tech-support', 'ocupation_ Transport-moving',\n",
       "       'ethnicity_ Amer-Indian-Eskimo', 'ethnicity_ Asian-Pac-Islander',\n",
       "       'ethnicity_ Black', 'ethnicity_ Other', 'ethnicity_ White',\n",
       "       'gender_ Female', 'gender_ Male', 'native_country_ Mexico',\n",
       "       'native_country_ United-States', 'native_country_other'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 213,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Db-swjELmvZd",
    "outputId": "85a67407-eb09-4985-b544-7db59639788d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.preprocessing.data.MinMaxScaler"
      ]
     },
     "execution_count": 210,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "scaler_filename = \"scaler.joblib\"\n",
    "joblib.dump(scaler, scaler_filename) \n",
    "\n",
    "\n",
    "# And now to load...\n",
    "scaler2 = joblib.load(scaler_filename) \n",
    "# scaler = joblib.load(scaler_filename) \n",
    "type(scaler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from joblib import load\n",
    "test2 = load(\"testSVM2.joblib\")\n",
    "test1 = load(\"testSVM1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>education</th>\n",
       "      <th>workclass</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>ocupation</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>native_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>160323.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Doctorate</td>\n",
       "      <td>Private</td>\n",
       "      <td>Married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age    fnlwgt  capital_gain  capital_loss  hours_per_week   education  \\\n",
       "0  40.0  160323.0           0.0           0.0             0.0   Doctorate   \n",
       "\n",
       "  workclass marital_status           ocupation ethnicity   gender  \\\n",
       "0   Private        Married   Machine-op-inspct     White   Female   \n",
       "\n",
       "   native_country  \n",
       "0   United-States  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>workclass_ Private</th>\n",
       "      <th>workclass_ Self-emp-inc</th>\n",
       "      <th>workclass_ Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>ethnicity_ Asian-Pac-Islander</th>\n",
       "      <th>ethnicity_ Black</th>\n",
       "      <th>ethnicity_ Other</th>\n",
       "      <th>ethnicity_ White</th>\n",
       "      <th>gender_ Female</th>\n",
       "      <th>gender_ Male</th>\n",
       "      <th>native_country_ Mexico</th>\n",
       "      <th>native_country_ United-States</th>\n",
       "      <th>native_country_other</th>\n",
       "      <th>Unnamed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30159</th>\n",
       "      <td>52</td>\n",
       "      <td>287927</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fnlwgt  capital_gain  capital_loss  hours_per_week  \\\n",
       "30159   52  287927         15024             0              40   \n",
       "\n",
       "       workclass_ Federal-gov  workclass_ Local-gov  workclass_ Private  \\\n",
       "30159                       0                     0                   0   \n",
       "\n",
       "       workclass_ Self-emp-inc  workclass_ Self-emp-not-inc   ...     \\\n",
       "30159                        1                            0   ...      \n",
       "\n",
       "       ethnicity_ Asian-Pac-Islander  ethnicity_ Black  ethnicity_ Other  \\\n",
       "30159                              0                 0                 0   \n",
       "\n",
       "       ethnicity_ White  gender_ Female  gender_ Male  native_country_ Mexico  \\\n",
       "30159                 1               1             0                       0   \n",
       "\n",
       "       native_country_ United-States  native_country_other  Unnamed  \n",
       "30159                              1                     0    30159  \n",
       "\n",
       "[1 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator SVC from version 0.21.3 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model = load(\"svm_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in test2.columns:\n",
    "    test2[col].values[:] = 0\n",
    "model.predict(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de red_neuronal.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
