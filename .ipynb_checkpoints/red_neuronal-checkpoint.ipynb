{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal utilizando Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora construiremos una red neuronal utilizando la libreria de Keras para implementar tensorflow.\n",
    "Utilizaremos la base de datos sin valores faltantes para entrenar el modelo, y mediante validación cruzada probaremos su efectividad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import keras\n",
    "import os\n",
    "import qgrid\n",
    "import functools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/dataset_tae_final_no_na_mod.csv')\n",
    "test = pd.read_csv('data/test_tae_no_na_mod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b6285efa8449628707a07a48efa6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QgridWidget(grid_options={'fullWidthRows': True, 'syncColumnCellResize': True, 'forceFitColumns': True, 'defau…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qqview = qgrid.show_grid(train)\n",
    "qqview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos nuestra variable objetivo y nuestras caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                int64\n",
       "workclass         object\n",
       "fnlwgt             int64\n",
       "education         object\n",
       "marital_status    object\n",
       "ocupation         object\n",
       "ethnicity         object\n",
       "gender            object\n",
       "capital_gain       int64\n",
       "capital_loss       int64\n",
       "hours_per_week     int64\n",
       "native_country    object\n",
       "income            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column = list(train.columns)\n",
    "features = column[:-1]\n",
    "label = column[-1]\n",
    "#Chequeo de tipos de las variables\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos variables categoricas, debemos transformarlas a variables dummies par apoder crear la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "catego_columns = ['education', 'workclass', 'marital_status', \n",
    "             'ethnicity', 'income','gender',\n",
    "                  'native_country', 'ocupation']\n",
    "#Transformamos variables object a categoricas\n",
    "for col in catego_columns:\n",
    "    train[col] = pd.Categorical(train[col])\n",
    "    test[col] = pd.Categorical(test[col])\n",
    "\n",
    "#Transformamos nuestra variable de objetivo para la clasificacion\n",
    "train[label] = train[label].cat.codes\n",
    "test[label] = test[label].cat.codes\n",
    "\n",
    "#Creamos variables dummies con las categoricas\n",
    "train_dataset=pd.get_dummies(train)\n",
    "test_dataset=pd.get_dummies(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chequeamos que las variables de nuestro conjunto de entrenamiento sean las mismas que las de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in list(train_dataset.columns):\n",
    "    if not((elem in list(test_dataset.columns))):\n",
    "        print(elem)\n",
    "#Everything is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'fnlwgt', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
       "       'income', 'workclass_ Federal-gov', 'workclass_ Local-gov',\n",
       "       'workclass_ Private', 'workclass_ Self-emp-inc',\n",
       "       'workclass_ Self-emp-not-inc', 'workclass_ State-gov',\n",
       "       'workclass_ Without-pay', 'education_ 10th', 'education_ 11th',\n",
       "       'education_ 12th', 'education_ 1st-4th', 'education_ 5th-6th',\n",
       "       'education_ 7th-8th', 'education_ 9th', 'education_ Assoc-acdm',\n",
       "       'education_ Assoc-voc', 'education_ Bachelors', 'education_ Doctorate',\n",
       "       'education_ HS-grad', 'education_ Masters', 'education_ Preschool',\n",
       "       'education_ Prof-school', 'education_ Some-college',\n",
       "       'marital_status_ Divorced', 'marital_status_ Never-married',\n",
       "       'marital_status_ Separated', 'marital_status_ Widowed',\n",
       "       'marital_status_Married', 'ocupation_ Adm-clerical',\n",
       "       'ocupation_ Armed-Forces', 'ocupation_ Craft-repair',\n",
       "       'ocupation_ Exec-managerial', 'ocupation_ Farming-fishing',\n",
       "       'ocupation_ Handlers-cleaners', 'ocupation_ Machine-op-inspct',\n",
       "       'ocupation_ Other-service', 'ocupation_ Priv-house-serv',\n",
       "       'ocupation_ Prof-specialty', 'ocupation_ Protective-serv',\n",
       "       'ocupation_ Sales', 'ocupation_ Tech-support',\n",
       "       'ocupation_ Transport-moving', 'ethnicity_ Amer-Indian-Eskimo',\n",
       "       'ethnicity_ Asian-Pac-Islander', 'ethnicity_ Black', 'ethnicity_ Other',\n",
       "       'ethnicity_ White', 'gender_ Female', 'gender_ Male',\n",
       "       'native_country_ Mexico', 'native_country_ United-States',\n",
       "       'native_country_other'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = train_dataset.columns\n",
    "columns\n",
    "#target = train_dataset.pop(\"income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente convertimos nuestra base de datos al formato de tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_dataset.copy(deep=True)\n",
    "target = df.pop('income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = tf.data.Dataset.from_tensor_slices((train_dataset.values, target.values))\n",
    "final_train = final_train.shuffle(len(train)).batch(64)\n",
    "type(final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-b48d9c01805f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_dummies.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data_set' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_set.to_csv('train_dummies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_compiled_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    38,  89814,      0,      0,     50,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    28, 336951,      0,      0,     40,      0,      1,      0,\n",
       "             0,      0],\n",
       "       [    44, 160323,   7688,      0,     40,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    34, 198693,      0,      0,     30,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    63, 104626,   3103,      0,     32,      0,      0,      0,\n",
       "             0,      1],\n",
       "       [    24, 369667,      0,      0,     40,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    55, 104996,      0,      0,     10,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    65, 184454,   6418,      0,     40,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    36, 212465,      0,      0,     40,      1,      0,      0,\n",
       "             0,      0],\n",
       "       [    26,  82091,      0,      0,     39,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    48, 279724,   3103,      0,     48,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    43, 346189,      0,      0,     50,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    20, 444554,      0,      0,     25,      0,      0,      0,\n",
       "             0,      0],\n",
       "       [    43, 128354,      0,      0,     30,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    37,  60548,      0,      0,     20,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    34, 107914,      0,      0,     47,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    34, 238588,      0,      0,     35,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    25, 220931,      0,      0,     43,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    25, 205947,      0,      0,     40,      0,      0,      1,\n",
       "             0,      0],\n",
       "       [    45, 432824,   7298,      0,     90,      0,      0,      0,\n",
       "             0,      1]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = train_dataset.copy(deep=True)\n",
    "x_val = test_dataset.copy(deep=True)\n",
    "y_train = x_train.pop('income')\n",
    "y_val = x_val.pop('income')\n",
    "temp = x_val.values\n",
    "temp[0:20,0:10] \n",
    "#val_data = tf.data.Dataset.from_tensor_slices((x_test.values, y_test.values))\n",
    "\n",
    "#model = get_compiled_model()\n",
    "\n",
    "#model.fit(final_train, validation_data=(x_val, y_val),epochs = 20, \n",
    "#          steps_per_epoch=20,validation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n"
     ]
    }
   ],
   "source": [
    "print(\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-4949d14cc9c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m ])\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m472\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 851\u001b[1;33m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    852\u001b[0m     elif distributed_training_utils.is_tpu_strategy(\n\u001b[0;32m    853\u001b[0m         self._distribution_strategy):\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1173\u001b[0m     \u001b[1;31m# Validate and standardize user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[1;32m-> 1175\u001b[1;33m         x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[0;32m   2297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2298\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2299\u001b[1;33m         raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[0;32m   2300\u001b[0m                            \u001b[1;34m'training/testing. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2301\u001b[0m                            'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])\n",
    "model.fit(final_train, epochs = 20, steps_per_epoch=472)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-65188d136b45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss test: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def loss(model, x, y):\n",
    "  y_ = model(x)\n",
    "  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
    "\n",
    "\n",
    "l = loss(model, features, target)\n",
    "print(\"Loss test: {}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'workclass',\n",
       " 'fnlwgt',\n",
       " 'education',\n",
       " 'marital_status',\n",
       " 'ocupation',\n",
       " 'ethnicity',\n",
       " 'gender',\n",
       " 'capital_gain',\n",
       " 'capital_loss',\n",
       " 'hours_per_week',\n",
       " 'native_country']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
