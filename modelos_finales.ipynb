{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copia de red_neuronal.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc5wHKBKCeoW",
        "colab_type": "text"
      },
      "source": [
        "# Informe modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKBAhz-aC_e7",
        "colab_type": "text"
      },
      "source": [
        "Por:\n",
        "- David Chaverra Munera  \n",
        "- Juan Felipe Munera Vergara  \n",
        "- Andres Felipe Aguilar Rendon    \n",
        "- Christian Camilo Guzmán Escobar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EidoI-_DARa",
        "colab_type": "text"
      },
      "source": [
        "En el siguiente documento se muestra el proceso de creación de cada uno de los 3 modelos propuestos para el trabajo finall: redes neuronales, maquinas de soporte vectorial y random forest. Utilizando la base de datos abiertos  [_Adult_](https://archive.ics.uci.edu/ml/datasets/Adult), prediciento la variable _income_. \n",
        "\n",
        "El análisis descriptivo y la lista de cambios realizados a la base de datos previo al entrenamiento de los modelos puede encontrarse en el archivo [Análisis descriptivo TAE.docx](https://github.com/DavidChaMun/TAE_FINAL/raw/master/An%C3%A1lisis%20descriptivo%20TAE.docx)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-YOCMxgCfFE",
        "colab_type": "text"
      },
      "source": [
        "## Redes neuronales:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_PFlBms_bz0",
        "colab_type": "text"
      },
      "source": [
        "Ahora construiremos una red neuronal utilizando la libreria de Keras que imprementa tensorflow.\n",
        "Utilizaremos la base de datos sin valores faltantes y con las modificaciones mencionadas previamente para entrenar el modelo. El modelo se elegira mediante validación cruzada, para probar su efectividad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDgVN0vl_bz1",
        "colab_type": "text"
      },
      "source": [
        "### Cargando librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0HOEBZP_bz2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f4c960c-7d5b-4cd4-a9a9-4dff0011d2cd"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "#tf.enable_eager_execution()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import os\n",
        "import functools\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_s3hfTTHqvU",
        "colab_type": "text"
      },
      "source": [
        "Cargamos la base de datos desde Github."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0Vho-SP_bz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('https://github.com/DavidChaMun/TAE_FINAL/raw/master/data/dataset_tae_final_no_na_mod.csv')\n",
        "test = pd.read_csv('https://github.com/DavidChaMun/TAE_FINAL/raw/master/data/test_tae_no_na_mod.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0k8H59A_b0D",
        "colab_type": "text"
      },
      "source": [
        "Observemos la estructura de los datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLCnuCIZ_b0E",
        "colab_type": "code",
        "outputId": "74e13942-49cf-4122-a7d8-c1b249f27c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#Chequeo de tipos de las variables\n",
        "train.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age                  int64\n",
              "workclass         category\n",
              "fnlwgt               int64\n",
              "education         category\n",
              "marital_status    category\n",
              "ocupation         category\n",
              "ethnicity         category\n",
              "gender            category\n",
              "capital_gain         int64\n",
              "capital_loss         int64\n",
              "hours_per_week       int64\n",
              "native_country    category\n",
              "income                int8\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_6aai8RV9SD",
        "colab_type": "text"
      },
      "source": [
        "## Transformación de la base de datos a un formato conveniente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0tGLQMQ_b0H",
        "colab_type": "text"
      },
      "source": [
        "Vemos que tenemos variables categóricas, debemos transformarlas a variables dummies para poder crear la red neuronal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygOZN5ay_b0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "catego_columns = ['education', 'workclass', 'marital_status', \n",
        "             'ethnicity', 'income','gender',\n",
        "                  'native_country', 'ocupation']\n",
        "#Transformamos variables object a categoricas\n",
        "for col in catego_columns:\n",
        "    train[col] = pd.Categorical(train[col])\n",
        "    test[col] = pd.Categorical(test[col])\n",
        "\n",
        "#Transformamos nuestra variable de objetivo para la clasificacion\n",
        "train[label] = train[label].cat.codes\n",
        "test[label] = test[label].cat.codes\n",
        "\n",
        "#Creamos variables dummies con las categoricas\n",
        "train_dataset=pd.get_dummies(train)\n",
        "test_dataset=pd.get_dummies(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjJpPyNS_b0K",
        "colab_type": "text"
      },
      "source": [
        "Chequeamos que las variables de nuestro conjunto de entrenamiento sean las mismas que las de validación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DvL0wTc_b0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for elem in list(train_dataset.columns):\n",
        "    if not((elem in list(test_dataset.columns))):\n",
        "        print(elem)\n",
        "#Dado que no imprime nada, las dos bases tienen las mismas variables dummies."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3kxDMNM_b0O",
        "colab_type": "code",
        "outputId": "3a1d1989-f003-4970-9104-6db9036fe23f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "columns = train_dataset.columns\n",
        "columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['age', 'fnlwgt', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
              "       'income', 'workclass_ Federal-gov', 'workclass_ Local-gov',\n",
              "       'workclass_ Private', 'workclass_ Self-emp-inc',\n",
              "       'workclass_ Self-emp-not-inc', 'workclass_ State-gov',\n",
              "       'workclass_ Without-pay', 'education_ 10th', 'education_ 11th',\n",
              "       'education_ 12th', 'education_ 1st-4th', 'education_ 5th-6th',\n",
              "       'education_ 7th-8th', 'education_ 9th', 'education_ Assoc-acdm',\n",
              "       'education_ Assoc-voc', 'education_ Bachelors', 'education_ Doctorate',\n",
              "       'education_ HS-grad', 'education_ Masters', 'education_ Preschool',\n",
              "       'education_ Prof-school', 'education_ Some-college',\n",
              "       'marital_status_ Divorced', 'marital_status_ Never-married',\n",
              "       'marital_status_ Separated', 'marital_status_ Widowed',\n",
              "       'marital_status_Married', 'ocupation_ Adm-clerical',\n",
              "       'ocupation_ Armed-Forces', 'ocupation_ Craft-repair',\n",
              "       'ocupation_ Exec-managerial', 'ocupation_ Farming-fishing',\n",
              "       'ocupation_ Handlers-cleaners', 'ocupation_ Machine-op-inspct',\n",
              "       'ocupation_ Other-service', 'ocupation_ Priv-house-serv',\n",
              "       'ocupation_ Prof-specialty', 'ocupation_ Protective-serv',\n",
              "       'ocupation_ Sales', 'ocupation_ Tech-support',\n",
              "       'ocupation_ Transport-moving', 'ethnicity_ Amer-Indian-Eskimo',\n",
              "       'ethnicity_ Asian-Pac-Islander', 'ethnicity_ Black', 'ethnicity_ Other',\n",
              "       'ethnicity_ White', 'gender_ Female', 'gender_ Male',\n",
              "       'native_country_ Mexico', 'native_country_ United-States',\n",
              "       'native_country_other'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUFbvvMo_b0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67c254b6-6c53-4168-bf85-1c16278ec1fa"
      },
      "source": [
        "#train_dataset.to_csv('train_dummies.csv')\n",
        "temp = ['age', 'fnlwgt', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
        "       'income', 'workclass_ Federal-gov', 'workclass_ Local-gov',\n",
        "       'workclass_ Private', 'workclass_ Self-emp-inc',\n",
        "       'workclass_ Self-emp-not-inc', 'workclass_ State-gov',\n",
        "       'workclass_ Without-pay', 'education_ 10th', 'education_ 11th',\n",
        "       'education_ 12th', 'education_ 1st-4th', 'education_ 5th-6th',\n",
        "       'education_ 7th-8th', 'education_ 9th', 'education_ Assoc-acdm',\n",
        "       'education_ Assoc-voc', 'education_ Bachelors', 'education_ Doctorate',\n",
        "       'education_ HS-grad', 'education_ Masters', 'education_ Preschool',\n",
        "       'education_ Prof-school', 'education_ Some-college',\n",
        "       'marital_status_ Divorced', 'marital_status_ Never-married',\n",
        "       'marital_status_ Separated', 'marital_status_ Widowed',\n",
        "       'marital_status_Married', 'ocupation_ Adm-clerical',\n",
        "       'ocupation_ Armed-Forces', 'ocupation_ Craft-repair',\n",
        "       'ocupation_ Exec-managerial', 'ocupation_ Farming-fishing',\n",
        "       'ocupation_ Handlers-cleaners', 'ocupation_ Machine-op-inspct',\n",
        "       'ocupation_ Other-service', 'ocupation_ Priv-house-serv',\n",
        "       'ocupation_ Prof-specialty', 'ocupation_ Protective-serv',\n",
        "       'ocupation_ Sales', 'ocupation_ Tech-support',\n",
        "       'ocupation_ Transport-moving', 'ethnicity_ Amer-Indian-Eskimo',\n",
        "       'ethnicity_ Asian-Pac-Islander', 'ethnicity_ Black', 'ethnicity_ Other',\n",
        "       'ethnicity_ White', 'gender_ Female', 'gender_ Male',\n",
        "       'native_country_ Mexico', 'native_country_ United-States',\n",
        "       'native_country_other']\n",
        "len(temp)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnSvTSC0_b0a",
        "colab_type": "text"
      },
      "source": [
        "## Creando la red neuronal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9LM5kz7PXPD",
        "colab_type": "text"
      },
      "source": [
        "Para la construcción de la red neuronal se utilizo el modelo *sequential* de Keras, la cuál ajusta una red neuronal (utilizando ciertos métodos que describiremos más adelante), dicha función se entrena utilizando *.fit()*, este método recibe un conjunto de entrenamiento y permite introducir un conjunto de validación, por lo que en cada iteración de entrenamiento se puede ver la precisión de la predicción para ambos conjuntos de datos. \n",
        "\n",
        "Se eligió como criterio de elección del modelo aquel que maximizara la precisión en las predicciones de la variables *income*. Para la elección del modelo se variaron los siguientes atributos:\n",
        "\n",
        "- Número de capas y neuronas dentro de *sequential*: aumentar el numero de neuronas a más de dos no mejoró los resultados, el número de neuronas mejoraba los resultados al aumentarlas por encimas de 10, al final se eligieron 2 capas internas con 30 y 20 neuronas respectivamentes.\n",
        "\n",
        "- Método de activación de la neurona de salida: se probo \"softmax\" que según la documentación de internet es eficiente en clasificación de 2 o multiples variables categóricas. Sin embargo, este mostro precisiones desastrozas por debajo de 0.30 en general. Por lo que se utilizo \"sigmoid\".\n",
        "\n",
        "- Optimizador: se probaron *SGD* (precisiones de hasta 0.83), *Adam* y *RMSprop*, variando el __learning rate (LR)__ en todos ellos, se encontro que *Adam* y *RMSprop* llegaban a resultados equivalente de hasta 0.85 de precisión, los LR entre 0.001 y 0.1 convergian siempre a una precisión de 0.85, por lo que se eligió 0.1 al entrenar la red más rapidamente.\n",
        "\n",
        "- Número de epochs: estos son definimos al usar *model.fit()*. Aumentarlos o disminuirlos no impacto mucho el modelo final, la neurona convergia para el 10avo epoch, se eligio 20 para visualizar un comportamiento más prolongado.\n",
        "\n",
        "### A continuación se muestra la red neuronal final construida:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk3ul1zaWLGn",
        "colab_type": "text"
      },
      "source": [
        "#### Definición de la red neuronal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33wE3WM4_b0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import adam\n",
        "\n",
        "#Función que crea la red\n",
        "def get_compiled_model():\n",
        "  #Modelo sequencial, posee las neuronas\n",
        "  model = Sequential([\n",
        "    Dense(40, input_shape=(x_train.shape[1:])),\n",
        "    Activation('relu'),\n",
        "    Dense(20),\n",
        "    Activation('relu'),\n",
        "    Dense(1),\n",
        "    Activation('sigmoid'), #última neuronal\n",
        "  ])\n",
        "  \n",
        "  #optimizador\n",
        "  opt= adam(lr=0.01,beta_1=0.9, beta_2=0.9)\n",
        "  \n",
        "  #definición del copiador\n",
        "  model.compile(optimizer=opt,\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksAk35dEWRTJ",
        "colab_type": "text"
      },
      "source": [
        "#### Escalado y definición de variables de apoyo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvK6XI3k6UhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#Definimos las variables de entrenamiento y las objetivo para entrenar el modelo\n",
        "x_train = train_dataset.copy(deep=True)\n",
        "x_val = test_dataset.copy(deep=True)\n",
        "y_train = x_train.pop('income').values\n",
        "y_val = x_val.pop('income').values\n",
        "\n",
        "#Escalamos los datos\n",
        "scaler = MinMaxScaler() #definimos nuestro escalador, nos será útil a la hora de convertir los datos para hacer las predicciones.\n",
        "scaler.fit(x_train)    \n",
        "x_train =  scaler.transform(x_train)\n",
        "x_val = scaler.transform(x_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp6mfoNcWXaG",
        "colab_type": "text"
      },
      "source": [
        "### Entrenamiento y validación de la red neuronal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX1Q9Ye4_b0e",
        "colab_type": "code",
        "outputId": "392c809b-f973-436e-ff4a-28bcbefd4ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "#Obtenemos nuestro modelo\n",
        "model = get_compiled_model()\n",
        "\n",
        "#Entrenamos el modelo.\n",
        "model.fit(x=x_train, y=y_train,validation_data=(x_val,y_val),epochs = 20,batch_size=20,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 30160 samples, validate on 15059 samples\n",
            "Epoch 1/20\n",
            "30160/30160 [==============================] - 4s 121us/step - loss: 0.3632 - acc: 0.8313 - val_loss: 0.3498 - val_acc: 0.8375\n",
            "Epoch 2/20\n",
            "30160/30160 [==============================] - 3s 83us/step - loss: 0.3440 - acc: 0.8404 - val_loss: 0.3464 - val_acc: 0.8335\n",
            "Epoch 3/20\n",
            "30160/30160 [==============================] - 3s 84us/step - loss: 0.3412 - acc: 0.8440 - val_loss: 0.3353 - val_acc: 0.8447\n",
            "Epoch 4/20\n",
            "30160/30160 [==============================] - 3s 84us/step - loss: 0.3386 - acc: 0.8457 - val_loss: 0.3348 - val_acc: 0.8465\n",
            "Epoch 5/20\n",
            "30160/30160 [==============================] - 2s 82us/step - loss: 0.3350 - acc: 0.8473 - val_loss: 0.3400 - val_acc: 0.8443\n",
            "Epoch 6/20\n",
            "30160/30160 [==============================] - 3s 84us/step - loss: 0.3353 - acc: 0.8488 - val_loss: 0.3373 - val_acc: 0.8475\n",
            "Epoch 7/20\n",
            "30160/30160 [==============================] - 3s 84us/step - loss: 0.3346 - acc: 0.8487 - val_loss: 0.3323 - val_acc: 0.8483\n",
            "Epoch 8/20\n",
            "30160/30160 [==============================] - 3s 84us/step - loss: 0.3339 - acc: 0.8507 - val_loss: 0.3367 - val_acc: 0.8477\n",
            "Epoch 9/20\n",
            "30160/30160 [==============================] - 3s 85us/step - loss: 0.3330 - acc: 0.8508 - val_loss: 0.3400 - val_acc: 0.8489\n",
            "Epoch 10/20\n",
            "30160/30160 [==============================] - 3s 85us/step - loss: 0.3330 - acc: 0.8507 - val_loss: 0.3350 - val_acc: 0.8491\n",
            "Epoch 11/20\n",
            "30160/30160 [==============================] - 3s 84us/step - loss: 0.3329 - acc: 0.8513 - val_loss: 0.3467 - val_acc: 0.8446\n",
            "Epoch 12/20\n",
            "30160/30160 [==============================] - 3s 85us/step - loss: 0.3329 - acc: 0.8518 - val_loss: 0.3405 - val_acc: 0.8491\n",
            "Epoch 13/20\n",
            "30160/30160 [==============================] - 3s 84us/step - loss: 0.3350 - acc: 0.8521 - val_loss: 0.3464 - val_acc: 0.8479\n",
            "Epoch 14/20\n",
            "30160/30160 [==============================] - 3s 85us/step - loss: 0.3329 - acc: 0.8523 - val_loss: 0.3494 - val_acc: 0.8384\n",
            "Epoch 15/20\n",
            "30160/30160 [==============================] - 3s 85us/step - loss: 0.3342 - acc: 0.8528 - val_loss: 0.3606 - val_acc: 0.8477\n",
            "Epoch 16/20\n",
            "30160/30160 [==============================] - 3s 85us/step - loss: 0.3351 - acc: 0.8524 - val_loss: 0.3440 - val_acc: 0.8437\n",
            "Epoch 17/20\n",
            "30160/30160 [==============================] - 3s 84us/step - loss: 0.3347 - acc: 0.8528 - val_loss: 0.3480 - val_acc: 0.8485\n",
            "Epoch 18/20\n",
            "30160/30160 [==============================] - 3s 84us/step - loss: 0.3334 - acc: 0.8540 - val_loss: 0.3451 - val_acc: 0.8474\n",
            "Epoch 19/20\n",
            "30160/30160 [==============================] - 3s 83us/step - loss: 0.3350 - acc: 0.8542 - val_loss: 0.3411 - val_acc: 0.8481\n",
            "Epoch 20/20\n",
            "30160/30160 [==============================] - 2s 83us/step - loss: 0.3357 - acc: 0.8538 - val_loss: 0.3407 - val_acc: 0.8502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff5d11160f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyfzJTfrV7O9",
        "colab_type": "text"
      },
      "source": [
        "Nuestro modelo final tiene una precisión del **~85%** en los datos de validación. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUTi7pwJW2wz",
        "colab_type": "text"
      },
      "source": [
        "### Demo de nuevas predicciones\n",
        "\n",
        "Ahora construiremos una función que reciba una base numpy con los datos a predecir, construya las variables dummies, las escale y sea predicho por el modelo final.\n",
        "\n",
        "Para conseguir la misma estructura de la base de datos con la que se entreno el modelo, basta con crear un pd.data_frame con valores de un _input valido_, y añadir las columnas de la df con dummies que falten con valores con cero. Luego se escala con el objeto _scaler_ con el que se escaló la base original. \n",
        "\n",
        "Una clase que realiza este proceso cargando la red neuronal y el scaler se presenta a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_DG-ceXGVOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Librerias necesarias:\n",
        "from sklearn.externals import joblib\n",
        "from keras.models import load_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "\n",
        "\n",
        "class neural_network_mod():\n",
        "\n",
        "    scaler =None\n",
        "    nn_model = None\n",
        "    train_cols = ['age', 'fnlwgt', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
        "       'workclass_ Federal-gov', 'workclass_ Local-gov', 'workclass_ Private',\n",
        "       'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc',\n",
        "       'workclass_ State-gov', 'workclass_ Without-pay', 'education_ 10th',\n",
        "       'education_ 11th', 'education_ 12th', 'education_ 1st-4th',\n",
        "       'education_ 5th-6th', 'education_ 7th-8th', 'education_ 9th',\n",
        "       'education_ Assoc-acdm', 'education_ Assoc-voc', 'education_ Bachelors',\n",
        "       'education_ Doctorate', 'education_ HS-grad', 'education_ Masters',\n",
        "       'education_ Preschool', 'education_ Prof-school',\n",
        "       'education_ Some-college', 'marital_status_ Divorced',\n",
        "       'marital_status_ Never-married', 'marital_status_ Separated',\n",
        "       'marital_status_ Widowed', 'marital_status_Married',\n",
        "       'ocupation_ Adm-clerical', 'ocupation_ Armed-Forces',\n",
        "       'ocupation_ Craft-repair', 'ocupation_ Exec-managerial',\n",
        "       'ocupation_ Farming-fishing', 'ocupation_ Handlers-cleaners',\n",
        "       'ocupation_ Machine-op-inspct', 'ocupation_ Other-service',\n",
        "       'ocupation_ Priv-house-serv', 'ocupation_ Prof-specialty',\n",
        "       'ocupation_ Protective-serv', 'ocupation_ Sales',\n",
        "       'ocupation_ Tech-support', 'ocupation_ Transport-moving',\n",
        "       'ethnicity_ Amer-Indian-Eskimo', 'ethnicity_ Asian-Pac-Islander',\n",
        "       'ethnicity_ Black', 'ethnicity_ Other', 'ethnicity_ White',\n",
        "       'gender_ Female', 'gender_ Male', 'native_country_ Mexico',\n",
        "       'native_country_ United-States', 'native_country_other']\n",
        "    catego_columns = ['education', 'workclass', 'marital_status', \n",
        "             'ethnicity','gender',\n",
        "                  'native_country', 'ocupation']\n",
        "    \n",
        "    def __init__(self):\n",
        "        #Carga el scaler y la red neuronal:\n",
        "        self.scaler = joblib.load('scaler.save')\n",
        "        self.nn_model = load_model('my_model.h5')\n",
        "        \n",
        "    def predict(self, data):\n",
        "      #Esta función s eutiliza para predecir, recibe data un pd_dataframe\n",
        "      \n",
        "      #Transformamos la base de datos\n",
        "      data = self.tidy_data(data)\n",
        "      \n",
        "      predictions = self.nn_model.predict(data)>0.5\n",
        "      predictions=np.where(predictions==0, \"<=50k\", predictions)\n",
        "      predictions=np.where(predictions=='True', \">50k\", predictions)\n",
        "      return(predictions)\n",
        "      \n",
        "    def tidy_data(self,data):\n",
        "      \n",
        "      #Transformamos variables object a categoricas\n",
        "      for col in self.catego_columns:        \n",
        "        data[col] = pd.Categorical(data[col])\n",
        "      \n",
        "      #Creamos variables dummies con las categoricas\n",
        "      data_d=pd.get_dummies(data)\n",
        "      \n",
        "      #Rellenamos con las variables dummies\n",
        "      missing_cols = set(self.train_cols) - set(data_d.columns)\n",
        "      \n",
        "      for c in missing_cols:\n",
        "        data_d[c] = 0\n",
        "        \n",
        "      # Ensure the order of column in the test set is in the same order than in train set\n",
        "      data_d = data_d[self.train_cols]\n",
        "      \n",
        "      #Escalamos los datos\n",
        "      data_d = self.scaler.transform(data_d)    \n",
        "      return(data_d)\n",
        "      \n",
        " \n",
        "\n",
        "\n",
        "ob = neural_network_mod()\n",
        "ob.predict(new_pred)\n",
        "#model.predict(x_train[[1,2],:])>0.5\n",
        "#y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "sv7oSmnkAyMd",
        "colab_type": "text"
      },
      "source": [
        "## Prediccion utilizando Maquinas de soporte vectorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwuCWrdbAyMf",
        "colab_type": "text"
      },
      "source": [
        "Utilizamos scikit learn e importamos todas las librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "rgzObo3hAyMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import svm, metrics\n",
        "from joblib import dump\n",
        "from google.colab import drive\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_V7KrblGAyMi",
        "colab_type": "text"
      },
      "source": [
        "Importamos las bases de datos necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "2ZNT8j1lAyMi",
        "colab_type": "code",
        "outputId": "a3087b88-727d-4d8c-9e80-e0d50d91d63e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "path_shared_folder = \"/content/drive/My Drive/Colab Notebooks/data\"\n",
        "os.chdir(path_shared_folder)\n",
        "\n",
        "train_data = pd.read_csv('train_dummies.csv')\n",
        "test_data = pd.read_csv('test_dummies.csv')\n",
        "train_answers = train_data.pop(\"income\")\n",
        "test_answers = test_data.pop(\"income\")\n",
        "\"listo\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'listo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "rxwrjIcJAyMl",
        "colab_type": "text"
      },
      "source": [
        "### Entrenamiento del modelo\n",
        "\n",
        "Para la construcción del modelo de máquina de vectores de soporte se utilizó el modelo la librería scikit learn. Se crea una variable clasificador ‘clf’ con un kernel específico, que tiene los métodos fit() para entrenar y predict() para evaluar el modelo.\n",
        "\n",
        "El kernel se eligió probando los diferentes kernels “linear”, “poly” y “rbf”. El rendimiento para el kernel “poly” dificultó sus pruebas dado que para incluso una muestra de 100 elementos, demoró más de 40 minutos entrenando. El modelo construido con el kernel “rbf” tuvo una precisión de aproximadamente 75%, menor a la precisión con el kernel “linear” que fue apróximadamente 78%.\n",
        "\n",
        "Los demás parámetros como gamma y C presentaron también problemas de rendimiento al ser cambiados. Posiblemente la precisión e incluso el rendimiento del entrenamiento del modelo podrían haber mejorado usando una librería como GridSearchCV para tunear los parámetros pero esto no se llevó a cabo por falta de tiempo. La precisión final del modelo fue identificada usando el método metrics.accuracy_score()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "8Sk3ABWyAyMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lista = list()\n",
        "#Create a svm Classifier\n",
        "clf = svm.SVC(kernel='linear')\n",
        "\n",
        "#Train the model using the training sets\n",
        "clf.fit(train_data, train_answers)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(test_data)\n",
        "lista.append(metrics.accuracy_score(test_answers, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy8e_7OCAyMo",
        "colab_type": "code",
        "outputId": "b32b1329-357a-4cf6-9463-d2769deb3a78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dump(clf, \"svm_model.joblib\")\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "\"Accuracy:\",lista"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Accuracy:', [0.7844478385018926])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srA09r7jRKQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5c39LczQi4p",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest\n",
        "Para la implementación de este método, se hace uso del módulo DecisionTreeClassifier de sklearn para el motor de predicción, además se hace uso del módulo LabelEncoder y OneHotEncoder para la generación de las variables dummies a partir de las variables categóricas. Al final con una matriz de confusión se podrá observar la precisión del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRViJpeMQi4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cargando Dependencias, Estas se pueden encontrar en requirements.txt\n",
        "import re\n",
        "import pandas as pd \n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.ensemble.forest import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oRQq1snQi4x",
        "colab_type": "text"
      },
      "source": [
        "### Base de datos\n",
        "Se hace uso de la base obtenida en https://archive.ics.uci.edu/ml/datasets/Adult, que busca predecir si los ingresos de un adulto son o no mayores a 50k."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuH4GXDTQi4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Se leen las bases de datos, tanto de entrenamiento como de prueba\n",
        "train_data_set = pd.read_csv(\"data/dataset_tae_final_no_na_mod.csv\", encoding = \"ISO-8859-1\")\n",
        "test_data_set = pd.read_csv(\"data/test_tae_no_na_mod.csv\", encoding = \"ISO-8859-1\")\n",
        "\n",
        "# Se hace una limpieza de la base de datos deprueba, pues presenta una diferencia en el formato de la variable a predecir\n",
        "test_data_set[\"income\"]=test_data_set[\"income\"].replace(\" <=50K.\", \" <=50K\")\n",
        "test_data_set[\"income\"]=test_data_set[\"income\"].replace(\" >50K.\", \" >50K\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8ycTxyjQi43",
        "colab_type": "text"
      },
      "source": [
        "### El modelo predictor\n",
        "La clase TAERandomForestClassifier sera el modelo, al crear una instancia de esta clase el modelo se inicializa sin ser entrenado con unos meta-parámetros por defecto para su entrenamiento, que en este caso ya son los óptimos: Esto se determinó mediante un ciclo que prueba cada uno de ellos en un rango, guarda sus precisiones y escoge la más alta.\n",
        "\n",
        "### Metaparametros\n",
        "Se tienen en cuenta 3 meta parametros:  \n",
        "- Numero de estimadores: 100.  \n",
        "Este meta parámetro determina el numero interno de modelos de árbol que va a usar, se determina que 100 es un numero optimo ya que la razón de ganancia de precisión contra el tiempo de entrenamiento no es lo suficientemente significativo para el sacrificio en el desempeño del modelo\n",
        "- Numero de categorias: 7.  \n",
        "Este meta parámetro determina cuantas características de la base de datos va a usar por modelo como máximo\n",
        "- Maximo de profundidad: 16.  \n",
        "Este meta parámetro determina el máximo de niveles que los arboles pueden tener al ser generados en el bosque\n",
        "\n",
        "\n",
        "### Metodos\n",
        "La clase usa 6 métodos, 2 métodos para la codificación de las variables categóricas en dummies, donde encode_fit hace la categorización en el entrenamiento y entrega un diccionario para futuras categorizaciones de dummies. El método encode hace la categorización en dummies de las futuras entradas. Tenemos un metodo fit, que toma una base de datos con la cual se entrena el modelo, luego un método predict que usa el modelo entrenado para predecir con entradas nuevas, ambos métodos reciben la base de datos en formato panda. Por último, tenemos el método cal_conf_matrix que nos imprime la matriz de confusión con base en una base de datos de prueba y la precisión de nuestro modelo, por último,\n",
        "tenemos set_meta que nos permite modificar los meta-parámetros del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Uq2dmIqQi44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TAERandomForestClassifier(object):\n",
        "    lab_encoders = {}\n",
        "    dummy_encoder = None\n",
        "    rfc_model = None\n",
        "    n_estimators = 100\n",
        "    max_features = 5\n",
        "    max_depth = 16\n",
        "    \n",
        "    def encode_fit(self, cat_data):\n",
        "        #Encodes string to numeric labels\n",
        "        tdc_set_encoded = cat_data.copy(deep=True)\n",
        "        for cn in cat_data.columns:\n",
        "            self.lab_encoders[cn] = preprocessing.LabelEncoder()\n",
        "            self.lab_encoders[cn].fit(cat_data[str(cn)])\n",
        "            tdc_set_encoded[str(cn)] = self.lab_encoders[cn].transform(cat_data[str(cn)])\n",
        "        \n",
        "        #Encodes to dummy dataset\n",
        "        self.dummy_encoder = preprocessing.OneHotEncoder(categories=\"auto\")\n",
        "        self.dummy_encoder.fit(tdc_set_encoded[cat_data.columns])\n",
        "        \n",
        "        #print(len(self.dummy_encoder.get_feature_names()))\n",
        "        \n",
        "        encoded_cat_data = pd.DataFrame(data=self.dummy_encoder.transform(tdc_set_encoded).todense(), columns=self.dummy_encoder.get_feature_names())\n",
        "        return encoded_cat_data\n",
        "    \n",
        "    def encode(self, cat_data):\n",
        "        for cn in cat_data.columns:\n",
        "              cat_data[str(cn)] = self.lab_encoders[cn].transform(cat_data[str(cn)]) \n",
        "        \n",
        "        \n",
        "        #Encodes to dummy dataset\n",
        "        encoded_cat_data = pd.DataFrame(data=self.dummy_encoder.transform(cat_data).todense(), columns=self.dummy_encoder.get_feature_names())    \n",
        "        return encoded_cat_data       \n",
        "    def fit(self, x_train, y_train, cat_cols, num_cols):\n",
        "        #Separates dataset in categorical and numbers\n",
        "        x_train_num = x_train[num_cols].copy(deep=True)\n",
        "        x_train_cat = x_train[cat_cols].copy(deep=True)\n",
        "        \n",
        "        x_train_cat = self.encode_fit(x_train_cat)\n",
        "        \n",
        "        x_train_num.reset_index(drop=True, inplace=True)\n",
        "        x_train_cat.reset_index(drop=True, inplace=True)\n",
        "        \n",
        "        f_x_train = pd.concat([x_train_num, x_train_cat], axis=1)\n",
        "\n",
        "        self.rfc_model = RandomForestClassifier(n_estimators=self.n_estimators, criterion=\"entropy\", \n",
        "                                                max_features=self.max_features, max_depth=self.max_depth)\n",
        "        self.rfc_model = self.rfc_model.fit(f_x_train, y_train)\n",
        "        \n",
        "    def predict(self, x_predict, cat_cols, num_cols):\n",
        "        #Separates dataset in categorical and numbers\n",
        "        x_predict_num = x_predict[num_cols].copy(deep=True)\n",
        "        x_predict_cat = x_predict[cat_cols].copy(deep=True)\n",
        "        \n",
        "        x_predict_cat = self.encode(x_predict_cat)\n",
        "        f_x_predict = pd.concat([x_predict_num, x_predict_cat], axis=1)\n",
        "        y_pred = self.rfc_model.predict(f_x_predict)\n",
        "        return y_pred\n",
        "    \n",
        "    def cal_conf_matrix(self, x_test, y_test, catego_columns, numeric_cols):\n",
        "        y_pred = self.predict(x_test, catego_columns, numeric_cols)\n",
        "        # [[VP, FP], [FN, VN]]\n",
        "        print(\"Matriz de confusión:\")\n",
        "        print(metrics.confusion_matrix(y_test, y_pred))\n",
        "\n",
        "        #Correr varias veces y ver como varia. Basado en el indice de jaccard\n",
        "        print(\"Precisión:\", metrics.accuracy_score(y_test, y_pred))\n",
        "        \n",
        "        return metrics.accuracy_score(y_test, y_pred)\n",
        "        \n",
        "    def set_meta(self, max_features, n_estimators, max_depth):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HELeoqfTQi47",
        "colab_type": "code",
        "outputId": "b64f54ae-fa9f-4ef9-b75f-1f8119246a78",
        "colab": {}
      },
      "source": [
        "# Se una instancia del odelo, se entrega con los datos de entrenamiento y se valida con los datos de prueba\n",
        "catego_columns = ['education', 'workclass', 'marital_status', 'ocupation', 'ethnicity', 'gender', 'native_country']\n",
        "numeric_cols = ['age', 'fnlwgt', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
        "\n",
        "forest = TAERandomForestClassifier()\n",
        "forest.fit(train_data_set.loc[:,train_data_set.columns!=\"income\",],train_data_set[\"income\"], catego_columns, numeric_cols)\n",
        "m = forest.cal_conf_matrix(test_data_set.loc[:,test_data_set.columns!=\"income\",], test_data_set[\"income\"], catego_columns, numeric_cols)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matriz de confusión:\n",
            "[[10841   518]\n",
            " [ 1622  2078]]\n",
            "Precisión: 0.8578922903247228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj2KnBrOQi5A",
        "colab_type": "text"
      },
      "source": [
        "### Resultados Matriz de Confusión\n",
        "\n",
        "Real vs  Pedicha | <=50k | >50k \n",
        "---- | ---- | ---- \n",
        "<=50k | 10788 | 571 |\n",
        " >50k | 1561 | 2139 \n",
        "\n",
        "Logrando una precision del: 86%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NC-h4ZnRNXw",
        "colab_type": "text"
      },
      "source": [
        "### Resumen de resultados:\n",
        "\n",
        "Los resultados de Random Forest y Neural networks son muy similares, podrían considerarse equivalentes. El modelo con el desempeño inferior fue el de maquinas de vectores de soporte. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLjrMJ4oSC6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}